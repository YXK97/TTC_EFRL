import pathlib
import jax
import jax.numpy as jnp
import jax.random as jr
import numpy as np
import functools as ft

from typing import NamedTuple, Tuple, Optional, List
from abc import ABC, abstractmethod, abstractproperty

from matplotlib import pyplot as plt
from matplotlib.animation import FuncAnimation
from matplotlib.axes import Axes
from matplotlib.collections import LineCollection

from jax.lax import dynamic_slice_in_dim

from ..trainer.data import Rollout
from ..utils.graph import EdgeBlock, GetGraph, GraphsTuple
from ..utils.typing import Action, Array, Cost, Done, Info, Reward, State, AgentState
from ..utils.utils import tree_index, MutablePatchCollection, save_anim
from .base import MultiAgentEnv


class MVEEnvState(NamedTuple): # Multi Vehicles Environment
    agent: State
    goal: State
    obstacle: State

    @property
    def n_agent(self) -> int:
        return self.agent.shape[0]


MVEEnvGraphsTuple = GraphsTuple[State, MVEEnvState]


class MVE(MultiAgentEnv, ABC): # # Multi Vehicles Environment
    """暂时只能设置为所有ego都使用一种模型，所有obst都使用一种模型"""

    AGENT = 0
    GOAL = 1
    OBST = 2

    PARAMS = {
        "ego_lf": 0.905, # m
        "ego_lr": 1.305, # m
        "ego_bb_size": jnp.array([2.21, 1.48]), # bounding box的[width, height] m # TODO
        "comm_radius": 30,
        "n_obsts": 1,
        "obst_bb_size": jnp.array([4.18, 1.99]), # bounding box的[width, height] m # TODO
        "collide_extra_bias": 0.1, # 用于计算cost时避碰的margin m

        "default_state_range": jnp.array([-35., 35., -9., 9., 0., 360., -5., 30.]), # [x_l, x_u, y_l, y_u, theta_l, theta_u, v_l, v_u]
        "rollout_state_range": jnp.array([-35., 35., -9., 9., 0., 360., -5., 30.]), # rollout过程中xy坐标和theta的限制
        #"agent_init_state_range": jnp.array([25., 35., -7., 7.5, 150., 210., 0., 0.]), # 用于agent初始化的状态范围
        #"goal_state_range": jnp.array([-30., -20., -6., 6., 180., 180., 0., 0.]), # 随机生成goal时的状态范围
        #"obst_state_range": jnp.array([-15., 20., -7., 7.5, 0., 360., 0., 0.]), # 随机生成obstacle的状态范围

        "dist2goal_bias": 0.1, # 用于判断agent是否到达goal m

        "theta2goal_bias": 0.98 # 用于判断agent航向角是否满足goal的要求，即agent方向向量和goal方向向量夹角的cos是否大于0.98（是否小于10度）
    }
    PARAMS.update({
        "ego_radius": jnp.linalg.norm(PARAMS["ego_bb_size"]/2), # m
        "ego_L": PARAMS["ego_lf"]+PARAMS["ego_lr"] # m
    })
    if PARAMS["n_obsts"] > 0:
        assert "obst_bb_size" in PARAMS and PARAMS["obst_bb_size"].shape == (2,)
    PARAMS.update({"obst_radius": jnp.linalg.norm(PARAMS["obst_bb_size"]/2)})

    def __init__(
            self,
            num_agents: int,
            area_size: Optional[float] = None,
            max_step: int = 1280,
            max_travel: Optional[float] = None,
            dt: float = 0.05,
            params: dict = None
    ):
        area_size = MVE.PARAMS["default_state_range"][:4] if area_size is None else area_size
        params = MVE.PARAMS if params is None else params
        super(MVE, self).__init__(num_agents, area_size, max_step, max_travel, dt, params)

    @property
    def state_dim(self) -> int:
        return 4  # x:车辆x轴坐标（m）、y:车辆y轴坐标（m）、theta:车辆航向角（与x轴正向夹角，逆时针为正，degree）、v:质心速率（m/s）

    @property
    def node_dim(self) -> int:
        return 9  # state dim (4) + bb_size(2) + indicator(3): agent: 001, goal: 010, obstacle: 100, pad: 00-1

    @property
    def edge_dim(self) -> int:
        return 4  # state_diff: x_diff, y_diff, theta_diff, v_diff

    @property
    def action_dim(self) -> int:
        return 2  # v:车辆质心速率（m/s）、delta:前轮转角（前轮与车辆中轴线正方向的夹角，逆时针为正，角度）

    @abstractproperty
    def reward_min(self) -> float:
        pass

    @property
    def reward_max(self) -> float:
        return 0.5 # TODO，貌似可以

    @property
    def n_cost(self) -> int:
        return 6 # agent间碰撞(1) + agent-obstacle碰撞(1) + agent超出x轴范围(高+低，2) + agent超出y轴范围(高+低，2)

    @property
    def cost_components(self) -> Tuple[str, ...]:
        return "agent collisions", "obs collisions", "bound exceeds x low", "bound exceeds x high", \
                "bound exceeds y low", "bound exceeds y high"

    def reset(self, key: Array) -> GraphsTuple:
        """先生成obstacle，将obstacle视为agent，通过cost计算是否valid
        再生成agent和goal，将之前生成的obstacle还原为obstacle，利用cost计算是否valid"""
        state_low_idx = jnp.array([0,2,4])
        state_high_idx = jnp.array([1,3,5])

        if self.params["n_obsts"] > 0:
            # randomly generate obstacles
            def get_obst(inp):
                this_key, state_range, _ = inp
                use_key, this_key = jr.split(this_key, 2)
                return this_key, state_range, \
                        jr.uniform(use_key, (self.params["n_obsts"], self.state_dim),
                            minval=jnp.concatenate([obst_state_range[state_low_idx], jnp.zeros((1,))], axis=0),
                            maxval=jnp.concatenate([obst_state_range[state_high_idx], jnp.zeros((1,))], axis=0))

            def non_valid_obst(inp):
                "根据cost判断是否valid"
                _, _, this_candidates = inp
                empty_obsts = jnp.empty((0, self.state_dim))
                tmp_state = MVEEnvState(this_candidates, this_candidates, empty_obsts)
                tmp_graph = self.get_graph(tmp_state, obst_as_agent=True)
                cost = self.get_cost(tmp_graph)
                return jnp.max(cost) > -0.5

            def get_valid_obsts(state_range, key):
                use_key, this_key = jr.split(key, 2)
                # 速度均设置为0
                obst_candidates = jr.uniform(use_key, (self.params["n_obsts"], self.state_dim),
                                            minval=jnp.concatenate([state_range[state_low_idx], jnp.zeros((1,))], axis=0),
                                            maxval=jnp.concatenate([state_range[state_high_idx], jnp.zeros((1,))], axis=0))
                _, _, valid_obsts = jax.lax.while_loop(non_valid_obst, get_obst, (this_key, state_range, obst_candidates))
                return valid_obsts

            if "obst_state_range" in self.params and self.params["obst_state_range"] is not None:
                obst_state_range = self.params["obst_state_range"]
            else:
                obst_state_range = self.params["default_state_range"]
            obst_key, key = jr.split(key, 2)
            obsts = get_valid_obsts(obst_state_range, obst_key)
        else:
            obsts = jnp.empty((0, self.state_dim))

        # randomly generate agents and goals
        def get_agent_goal(inp):
            this_key, state_range, _, obsts = inp
            use_key, this_key = jr.split(this_key, 2)
            return this_key, state_range,  \
                    jr.uniform(use_key, (self.num_agents, self.state_dim),
                        minval=jnp.concatenate([state_range[state_low_idx], jnp.zeros((1,))], axis=0),
                        maxval=jnp.concatenate([state_range[state_high_idx], jnp.zeros((1,))], axis=0)), \
                    obsts

        def non_valid_agent_goal(inp):
            "根据cost判断是否valid"
            _, _, this_candidates, obsts = inp
            tmp_state = MVEEnvState(this_candidates, this_candidates, obsts)
            tmp_graph = self.get_graph(tmp_state)
            cost = self.get_cost(tmp_graph)
            return jnp.max(cost) > -0.5

        def get_valid_agent_goal(state_range, key, obsts):
            use_key, this_key = jr.split(key, 2)
            # 速度均设置为0
            target_candidates = jr.uniform(use_key, (self.num_agents, self.state_dim),
                                        minval=jnp.concatenate([state_range[state_low_idx], jnp.zeros((1,))], axis=0),
                                        maxval=jnp.concatenate([state_range[state_high_idx], jnp.zeros((1,))], axis=0))
            _, _, valid_targets, _ = jax.lax.while_loop(non_valid_agent_goal, get_agent_goal,
                                    (this_key, state_range, target_candidates, obsts))
            return valid_targets

        if "goal_state_range" in self.params and self.params["goal_state_range"] is not None:
            goal_state_range = self.params["goal_state_range"]
        else:
            goal_state_range = self.params["default_state_range"]
        goal_key, key = jr.split(key, 2)
        goals = get_valid_agent_goal(goal_state_range, goal_key, obsts)

        if "agent_init_state_range" in self.params:
            if self.params["agent_init_state_range"] is not None:
                agent_init_state_range = self.params["agent_init_state_range"]
            else:
                agent_init_state_range = self.params["default_state_range"]
        else:
            agent_init_state_range = self.params["default_state_range"]
        agent_key = key
        agents = get_valid_agent_goal(agent_init_state_range, agent_key, obsts)

        env_state = MVEEnvState(agents, goals, obsts)

        return self.get_graph(env_state)

    def agent_step_euler(self, agent_states: AgentState, action: Action) -> AgentState:
        assert action.shape == (self.num_agents, self.action_dim)
        assert agent_states.shape == (self.num_agents, self.state_dim)

        # 车辆自行车运动学模型
        xs = agent_states[:, 0] # m
        ys = agent_states[:, 1] # m
        thetas = agent_states[:, 2] # degree
        action_vs = action[:, 0] # km/h
        action_deltas = action[:, 1] # degree
        betas = jnp.atan(self.params["ego_lr"] * jnp.tan(action_deltas * jnp.pi / 180) / self.params["ego_L"])
        new_xs = xs + action_vs / 3.6 * jnp.cos(thetas * jnp.pi / 180 + betas) * self.dt
        new_ys = ys + action_vs / 3.6 * jnp.sin(thetas * jnp.pi / 180 + betas) * self.dt
        new_thetas = (thetas + action_vs / 3.6 * jnp.cos(betas) * jnp.tan(action_deltas * jnp.pi / 180)
                      / self.params["ego_L"] * self.dt * 180 / jnp.pi) % 360 # 将所有角度归一到[0, 360)上

        n_state_agent_new = jnp.concatenate([new_xs[:, None], new_ys[:, None], new_thetas[:, None], action_vs[:, None]], axis=1)
        assert n_state_agent_new.shape == (self.num_agents, self.state_dim)
        return self.clip_state(n_state_agent_new)

    def step(
            self, graph: MVEEnvGraphsTuple, action: Action, get_eval_info: bool = False
    ) -> Tuple[MVEEnvGraphsTuple, Reward, Cost, Done, Info]:
        # get information from graph
        agent_states = graph.type_states(type_idx=MVE.AGENT, n_type=self.num_agents)
        goals = graph.type_states(type_idx=MVE.GOAL, n_type=self.num_agents)
        obstacles = graph.type_states(type_idx=MVE.OBST, n_type=self.params["n_obsts"]) if self.params["n_obsts"] > 0 else None

        # calculate next graph
        action = self.transform_action(action)
        next_agent_states = self.agent_step_euler(agent_states, action)
        next_env_state = MVEEnvState(next_agent_states, goals, obstacles)
        info = {}

        # the episode ends when reaching max_episode_steps
        done = jnp.array(False)

        # calculate reward and cost
        reward = self.get_reward(graph, action)
        cost = self.get_cost(graph)

        return self.get_graph(next_env_state), reward, cost, done, info

    @abstractmethod
    def get_reward(self, graph: MVEEnvGraphsTuple, action: Action) -> Reward:
        pass

    @abstractmethod
    def get_cost(self, graph: MVEEnvGraphsTuple) -> Cost:
        pass

    def render_video(
            self,
            rollout: Rollout,
            video_path: pathlib.Path,
            Ta_is_unsafe=None,
            viz_opts: Optional[dict] = None,
            n_goals: Optional[int] = None,
            **kwargs
    ) -> None:
        n_goals = self.num_agents if n_goals is None else n_goals

        ax: Axes
        fig, ax = plt.subplots(1, 1, figsize=(20,
                                (self.area_size[3]+3-(self.area_size[2]-3))*20/(self.area_size[1]+3-(self.area_size[0]-3))+4)
                               , dpi=100)
        ax.set_xlim(self.area_size[0]-3, self.area_size[1]+3)
        ax.set_ylim(self.area_size[2]-3, self.area_size[3]+3)
        ax.set(aspect="equal")
        plt.axis("on")
        if viz_opts is None:
            viz_opts = {}

        # 画y轴方向的限制，即车道边界限制
        ax.axhline(y=self.area_size[2], linewidth=2, color='k')
        ax.axhline(y=self.area_size[3], linewidth=2, color='k')

        # plot the first frame
        T_graph = rollout.graph
        graph0 = tree_index(T_graph, 0)

        agent_color = "#0068ff"
        goal_color = "#2fdd00"
        obst_color = "#8a0000"
        edge_goal_color = goal_color

        # plot obstacles
        if self.params["n_obsts"] > 0:
            obsts_state_bbsize = graph0.type_nodes(type_idx=MVE.OBST, n_type=self.params["n_obsts"])[:, :6]  # [n_obsts, 6] x,y,theta,v,width,height
            obsts_pos = obsts_state_bbsize[:, :2]
            obsts_theta = obsts_state_bbsize[:, 2]
            obsts_bb_size = obsts_state_bbsize[:, 4:6]
            obsts_radius = jnp.linalg.norm(obsts_bb_size, axis=1)
            plot_obsts_arrow = [plt.Arrow(x=obsts_pos[i,0], y=obsts_pos[i,1],
                                          dx=jnp.cos(obsts_theta[i]*jnp.pi/180)*obsts_radius[i]/2,
                                          dy=jnp.sin(obsts_theta[i]*jnp.pi/180)*obsts_radius[i]/2,
                                          width=1, color=obst_color, alpha=1.0) for i in range(len(obsts_theta))]
            plot_obsts_rec = [plt.Rectangle(xy=tuple(obsts_pos[i,:]-obsts_bb_size[i,:]/2),
                                            width=obsts_bb_size[i,0], height=obsts_bb_size[i,1],
                                            angle=obsts_theta[i], rotation_point='center',
                                            color=obst_color, linewidth=0.0, alpha=0.6) for i in range(len(obsts_theta))]
            plot_obsts_cir = [plt.Circle(xy=(obsts_pos[i,0], obsts_pos[i,1]), radius=self.params["obst_radius"],
                                         color=obst_color, linewidth=0.0, alpha=0.3) for i in range(len(obsts_theta))]
            col_obsts = MutablePatchCollection(plot_obsts_arrow+plot_obsts_rec+plot_obsts_cir, match_original=True, zorder=5)
            ax.add_collection(col_obsts)

        # plot goals
        goals_state_bbsize = graph0.type_nodes(type_idx=MVE.GOAL, n_type=n_goals)[:, :6]
        goals_pos = goals_state_bbsize[:, :2]
        goals_theta = goals_state_bbsize[:, 2]
        goals_bb_size = goals_state_bbsize[:, 4:6]
        goals_radius = jnp.linalg.norm(goals_bb_size, axis=1)
        plot_goals_arrow = [plt.Arrow(x=goals_pos[i,0], y=goals_pos[i,1],
                                      dx=jnp.cos(goals_theta[i]*jnp.pi/180)*goals_radius[i]/2,
                                      dy=jnp.sin(goals_theta[i]*jnp.pi/180)*goals_radius[i]/2,
                                      width=goals_radius[i]/jnp.mean(obsts_radius),
                                      alpha=1.0, color=goal_color) for i in range(n_goals)]
        plot_goals_rec = [plt.Rectangle(xy=tuple(goals_pos[i,:]-goals_bb_size[i,:]/2),
                                        width=goals_bb_size[i,0], height=goals_bb_size[i,1],
                                        angle=goals_theta[i], rotation_point='center',
                                        color=goal_color, linewidth=0.0, alpha=0.6) for i in range(n_goals)]
        plot_goals_cir = [plt.Circle(xy=(goals_pos[i,0], goals_pos[i,1]), radius=self.params["ego_radius"],
                                     color=goal_color, linewidth=0.0, alpha=0.3) for i in range(n_goals)]
        col_goals = MutablePatchCollection(plot_goals_arrow+plot_goals_rec+plot_goals_cir, match_original=True, zorder=6)
        ax.add_collection(col_goals)

        # plot agents
        agents_state_bbsize = graph0.type_nodes(type_idx=MVE.AGENT, n_type=self.num_agents)[:, :6]
        agents_pos = agents_state_bbsize[:, :2]
        agents_theta = agents_state_bbsize[:, 2]
        agents_bb_size = agents_state_bbsize[:, 4:6]
        agents_radius = jnp.linalg.norm(agents_bb_size, axis=1)
        plot_agents_arrow = [plt.Arrow(x=agents_pos[i, 0], y=agents_pos[i, 1],
                                       dx=jnp.cos(agents_theta[i] * jnp.pi / 180) * agents_radius[i]/2,
                                       dy=jnp.sin(agents_theta[i] * jnp.pi / 180) * agents_radius[i]/2,
                                       width=agents_radius[i] / jnp.mean(obsts_radius),
                                       alpha=1.0, color=agent_color) for i in range(self.num_agents)]
        plot_agents_rec = [plt.Rectangle(xy=tuple(agents_pos[i,:]-agents_bb_size[i,:]/2),
                                         width=agents_bb_size[i,0], height=agents_bb_size[i,1],
                                         angle=agents_theta[i], rotation_point='center',
                                         color=agent_color, linewidth=0.0, alpha=0.6) for i in range(self.num_agents)]
        plot_agents_cir = [plt.Circle(xy=(agents_pos[i,0], agents_pos[i,1]), radius=self.params["ego_radius"],
                                      color=agent_color, linewidth=0.0, alpha=0.3) for i in range(self.num_agents)]
        col_agents = MutablePatchCollection(plot_agents_arrow+plot_agents_rec+plot_agents_cir, match_original=True, zorder=7)
        ax.add_collection(col_agents)

        # plot edges
        all_pos = graph0.states[:, :2]
        edge_index = np.stack([graph0.senders, graph0.receivers], axis=0)
        is_pad = np.any(edge_index == self.num_agents + n_goals + self.params["n_obsts"], axis=0)
        e_edge_index = edge_index[:, ~is_pad]
        e_start, e_end = all_pos[e_edge_index[0, :]], all_pos[e_edge_index[1, :]]
        e_lines = np.stack([e_start, e_end], axis=1)  # (e, n_pts, dim)
        e_is_goal = (self.num_agents <= graph0.senders) & (graph0.senders < self.num_agents + n_goals)
        e_is_goal = e_is_goal[~is_pad]
        e_colors = [edge_goal_color if e_is_goal[ii] else "0.2" for ii in range(len(e_start))]
        col_edges = LineCollection(e_lines, colors=e_colors, linewidths=2, alpha=0.5, zorder=3)
        ax.add_collection(col_edges)

        # texts
        text_font_opts = dict(
            size=16,
            color="k",
            family="sans-serif",
            weight="normal",
            transform=ax.transAxes,
        )
        cost_text = ax.text(0.02, 1.00, "Cost: 1.0\nReward: 1.0", va="bottom", **text_font_opts)
        if Ta_is_unsafe is not None:
            safe_text = [ax.text(0.99, 1.00, "Unsafe: {}", va="bottom", ha="right", **text_font_opts)]
        kk_text = ax.text(0.99, 1.04, "kk=0", va="bottom", ha="right", **text_font_opts)
        z_text = ax.text(0.5, 1.04, "z: []", va="bottom", ha="center", **text_font_opts)

        # add agent labels
        label_font_opts = dict(
            size=20,
            color="k",
            family="sans-serif",
            weight="normal",
            ha="center",
            va="center",
            transform=ax.transData,
            clip_on=True,
            zorder=8,
        )
        agent_labels = [ax.text(float(agents_pos[ii, 0]), float(agents_pos[ii, 1]), f"{ii}", **label_font_opts)
                        for ii in range(self.num_agents)]

        if "Vh" in viz_opts:
            Vh_text = ax.text(0.99, 0.99, "Vh: []", va="top", ha="right", **text_font_opts)

        # init function for animation
        def init_fn() -> List[plt.Artist]:
            return [col_obsts, col_goals, col_agents, col_edges, *agent_labels, cost_text, *safe_text, kk_text]

        def update(kk: int) -> List[plt.Artist]:
            graph = tree_index(T_graph, kk)
            n_pos_t = graph.states[:-1, :2] # 最后一个node是padding，不要
            n_theta_t = graph.states[:-1, 2]
            n_bb_size_t = graph.nodes[:-1, 4:6]
            n_radius = jnp.linalg.norm(n_bb_size_t, axis=1)

            # update agents' positions and labels
            for ii in range(self.num_agents):
                plot_agents_arrow[ii].set_data(x=n_pos_t[ii, 0], y=n_pos_t[ii, 1],
                                               dx=jnp.cos(n_theta_t[ii]*jnp.pi/180)*n_radius[ii]/2,
                                               dy=jnp.sin(n_theta_t[ii]*jnp.pi/180)*n_radius[ii]/2)
                plot_agents_rec[ii].set_xy(xy=tuple(n_pos_t[ii, :]-n_bb_size_t[ii, :]/2))
                plot_agents_rec[ii].set_angle(angle=n_theta_t[ii])
                plot_agents_cir[ii].set_center(xy=tuple(n_pos_t[ii, :]))
                agent_labels[ii].set_position(n_pos_t[ii, :])
            # update goals' positions
            for ii in range(n_goals):
                plot_goals_arrow[ii].set_data(x=n_pos_t[self.num_agents+ii, 0], y=n_pos_t[self.num_agents+ii, 1],
                                              dx=jnp.cos(n_theta_t[self.num_agents+ii]*jnp.pi/180)*n_radius[self.num_agents+ii]/2,
                                              dy=jnp.sin(n_theta_t[self.num_agents+ii]*jnp.pi/180)*n_radius[self.num_agents+ii]/2)
                plot_goals_rec[ii].set_xy(xy=tuple(n_pos_t[self.num_agents+ii, :]-n_bb_size_t[self.num_agents+ii, :]/2))
                plot_goals_rec[ii].set_angle(angle=n_theta_t[self.num_agents+ii])
                plot_goals_cir[ii].set_center(xy=tuple(n_pos_t[self.num_agents+ii, :]))
            # update obstacles' positions
            if self.params["n_obsts"] > 0:
                for ii in range(self.params["n_obsts"]):
                    plot_obsts_arrow[ii].set_data(x=n_pos_t[self.num_agents+n_goals+ii, 0],
                                                  y=n_pos_t[self.num_agents+n_goals+ii, 1],
                                                  dx=jnp.cos(n_theta_t[self.num_agents+n_goals+ii]*jnp.pi/180)*n_radius[
                                                      self.num_agents+n_goals+ii]/2,
                                                  dy=jnp.sin(n_theta_t[self.num_agents+n_goals+ii]*jnp.pi/180)*n_radius[
                                                      self.num_agents+n_goals+ii]/2)
                    plot_obsts_rec[ii].set_xy(xy=tuple(n_pos_t[self.num_agents+n_goals+ii, :]-n_bb_size_t[self.num_agents+n_goals+ii, :]/2))
                    plot_obsts_rec[ii].set_angle(angle=n_theta_t[self.num_agents+n_goals+ii])
                    plot_obsts_cir[ii].set_center(xy=tuple(n_pos_t[self.num_agents+n_goals+ii, :]))

            # update edges
            e_edge_index_t = np.stack([graph.senders, graph.receivers], axis=0)
            is_pad_t = np.any(e_edge_index_t == self.num_agents + n_goals + self.params["n_obsts"], axis=0)
            e_edge_index_t = e_edge_index_t[:, ~is_pad_t]
            e_start_t, e_end_t = n_pos_t[e_edge_index_t[0, :]], n_pos_t[e_edge_index_t[1, :]]
            e_is_goal_t = (self.num_agents <= graph.senders) & (graph.senders < self.num_agents + n_goals)
            e_is_goal_t = e_is_goal_t[~is_pad_t]
            e_colors_t = [edge_goal_color if e_is_goal_t[ii] else "0.2" for ii in range(len(e_start_t))]
            e_lines_t = np.stack([e_start_t, e_end_t], axis=1)
            col_edges.set_segments(e_lines_t)
            col_edges.set_colors(e_colors_t)

            # update cost and safe labels
            if kk < len(rollout.costs):
                all_costs = ""
                for i_cost in range(rollout.costs[kk].shape[1]):
                    all_costs += f"    {self.cost_components[i_cost]}: {rollout.costs[kk][:, i_cost].max():5.4f}\n"
                all_costs = all_costs[:-2]
                cost_text.set_text(f"Cost:\n{all_costs}\nReward: {rollout.rewards[kk]:5.4f}")
            else:
                cost_text.set_text("")
            if kk < len(Ta_is_unsafe):
                a_is_unsafe = Ta_is_unsafe[kk]
                unsafe_idx = np.where(a_is_unsafe)[0]
                safe_text[0].set_text("Unsafe: {}".format(unsafe_idx))
            else:
                safe_text[0].set_text("Unsafe: {}")

            kk_text.set_text("kk={:04}".format(kk))

            # Update the z text.
            z_text.set_text(f"z: {rollout.zs[kk]}")

            if "Vh" in viz_opts:
                Vh_text.set_text(f"Vh: {viz_opts['Vh'][kk]}")

            return [col_obsts, col_goals, col_agents, col_edges, *agent_labels, cost_text, *safe_text, kk_text]

        fps = 30.0
        spf = 1 / fps
        mspf = 1_000 * spf
        anim_T = len(T_graph.n_node)
        ani = FuncAnimation(fig, update, frames=anim_T, init_func=init_fn, interval=mspf, blit=True)
        save_anim(ani, video_path)

    @abstractmethod
    def edge_blocks(self, state: MVEEnvState) -> List[EdgeBlock]:
        pass

    def get_graph(self, env_state: MVEEnvState, obst_as_agent:bool = False) -> MVEEnvGraphsTuple:
        num_agents = env_state.agent.shape[0]
        num_goals = env_state.goal.shape[0]
        num_obsts = env_state.obstacle.shape[0] # TODO: 为0时报错，但理论上可以为0
        assert num_agents > 0 and num_goals > 0, "至少需要设定agent和goal!"
        assert num_agents == num_goals, "每一个agent对应一个goal"
        # node features
        # states
        node_feats = jnp.zeros((num_agents + num_goals + num_obsts, self.node_dim))
        node_feats = node_feats.at[:num_agents, :self.state_dim].set(env_state.agent)
        node_feats = node_feats.at[num_agents: num_agents + num_goals, :self.state_dim].set(env_state.goal)
        if num_obsts > 0:
            node_feats = node_feats.at[num_agents + num_goals:, :self.state_dim].set(env_state.obstacle)

        # bounding box长宽
        if obst_as_agent:
            node_feats = node_feats.at[:num_agents + num_goals, 4:6].set(self.params["obst_bb_size"])
        else:
            node_feats = node_feats.at[:num_agents + num_goals, 4:6].set(self.params["ego_bb_size"])
        if num_obsts > 0:
            node_feats = node_feats.at[num_agents + num_goals:, 4:6].set(self.params["obst_bb_size"])

        # indicators
        node_feats = node_feats.at[:num_agents, 8].set(1.0)
        node_feats = node_feats.at[num_agents: num_agents + num_goals, 7].set(1.0)
        if num_obsts > 0:
            node_feats = node_feats.at[num_agents + num_goals:, 6].set(1.0)

        # node type
        node_type = -jnp.ones((num_agents + num_goals + num_obsts), dtype=jnp.int32)
        node_type = node_type.at[:num_agents].set(MVE.AGENT)
        node_type = node_type.at[num_agents: num_agents + num_goals].set(MVE.GOAL)
        if num_obsts > 0:
            node_type = node_type.at[num_agents + num_goals:].set(MVE.OBST)

        # edges
        edge_blocks = self.edge_blocks(env_state)

        # create graph
        states = jnp.concatenate([env_state.agent, env_state.goal], axis=0)
        if num_obsts > 0:
            states = jnp.concatenate([states, env_state.obstacle], axis=0)
        return GetGraph(node_feats, node_type, edge_blocks, env_state, states).to_padded()

    @abstractmethod
    def state_lim(self, state: Optional[State] = None) -> Tuple[State, State]:
        pass

    def action_lim(self) -> Tuple[Action, Action]:
        lower_lim = jnp.array([-5., -30.])[None, :].repeat(self.num_agents, axis=0) # v(允许倒车), delta
        upper_lim = jnp.array([30., 30.])[None, :].repeat(self.num_agents, axis=0)
        return lower_lim, upper_lim

    @ft.partial(jax.jit, static_argnums=(0,))
    def unsafe_mask(self, graph: GraphsTuple) -> Array:
        cost = self.get_cost(graph)
        return jnp.any(cost >= 0.0, axis=-1)